# Overview
A couple utilities for doing semi-automated duplicate file cleanup.

`find_dupes.py` is based on this [StackOverflow script](https://stackoverflow.com/a/36113168), modified to output as a JSON
object with all duplicates collated.

`prune_dupes.py` uses the json report generated by `find_dupes.py` to actually remove files from duplicate paths.

## `find_dupes.py` Usage:
Usage is pretty straight forward:

`find_dupes.py /path/to/find/dupes > duplicates.json`

## `prune_dupes.py` Usage:

### Always have a backup of your files.

While `prune_dupes.py` does validation to make sure it doesn't delete any unexpected files, always make sure you have
proper backups of your files before running this script.

|Option|Description|Multiple?|
|---|---|---|
|`--keep PATH`|Path to keep|Yes|
|`--prune PATH`|Path to prune|Yes|
|`--dupereport PATH`|Path to duplicates report| |
|`--delete`|Flag to enable actual deletion of files| |

### Backup your files.

Example without actual file deletion:

`./prune_dupes.py --keep /path/to/keep --keep /another/path/to/keep --prune /prunable/path/to/remove/files --prune /anotherprunable/path/to/remove/files --dupereport /path/to/report.json`

This will print out the script's intention for each file matching the keep/prune paths.  After reviewing the output you
can then add the `--delete` flag.

### Seriously, backup your files before trying to delete anything.
